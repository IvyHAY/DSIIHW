---
title: "HW1"
author: "Aiying Huang"
date: "2024-02-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
```

read in the data
```{r}
training_data <- read.csv("./HW1/housing_training.csv")
testing_data <- read.csv("./HW1/housing_test.csv")
```

# (a) Fit a lasso model on the training data. Report the selected tuning parameter and the test error. When the 1SE rule is applied, how many predictors are included in the model?

```{r}
# matrix of predictors (glmnet uses input matrix)
x <- model.matrix(Sale_Price ~ ., training_data)[,-1]
# vector of response
y <- training_data[, "Sale_Price"]

corrplot(cor(x), method = "circle", type = "full",tl.cex = 0.5)
```

```{r}
cv.lasso <- cv.glmnet(x, y, 
                      alpha = 1, 
                      nfolds = 10,
                      lambda = exp(seq(9, -5, length = 100)))

optimal_lambda <- cv.lasso$lambda.min  # Lambda that gives minimum mean cross-validated error
lambda_1se <- cv.lasso$lambda.1se  # Lambda within 1SE of the minimum
```


```{r}
plot(cv.lasso)
```
When the selected tuning parameter is 49.85361, the test error is 21018.03.
```{r}
x_test <- model.matrix(Sale_Price ~ ., testing_data)[, -1]
y_test <- testing_data$Sale_Price

lasso_model <- glmnet(x, y, alpha = 1, lambda = optimal_lambda)
predictions <- predict(lasso_model, s = optimal_lambda, newx = x_test)

test_error <- sqrt(mean((predictions - y_test)^2) ) # rmse
print(test_error)
```


```{r}
lasso_model_1se <- glmnet(x, y, alpha = 1, lambda = lambda_1se)
nonzero_coefficients <- sum(coef(lasso_model_1se, s = lambda_1se) != 0) - 1  # Subtracting 1 for the intercept
print(nonzero_coefficients)
```

When the 1SE rule is applied, 31 predictors are included in the model.

# (b) Fit an elastic net model on the training data. Report the selected tuning parameters and the test error. Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why.

```{r}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(2)
elasticnet.fit <- train(x, y ,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                          lambda = exp(seq(9, -5, length = 100))),
                   trControl = ctrl1)
```
```{r}
best_tune_caret <- elasticnet.fit$bestTune
alpha <- best_tune_caret$alpha
lambda <- best_tune_caret$lambda
best_tune_caret
```
The selected tuning parameters are alpha=0.15, lambda=272.0703. And the test error is 20995.

```{r}
elasticnet_model <- glmnet(x, y, alpha =alpha, lambda = lambda)
predictions <- predict(elasticnet_model, newx = x_test)

test_error <- sqrt(mean((predictions - y_test)^2) ) # RMSE
print(test_error)
```

It's possible to apply the 1SE rule:

```{r}
results <-elasticnet.fit$results
min_error <- min(results$RMSE)
min_error_std_error <- results$RMSESD[which.min(results$RMSE)]
```

```{r}
lambda_within_1se <- results$lambda[results$RMSE <= min_error + min_error_std_error]
lambda_1se <- max(lambda_within_1se)  # most regularized model within 1SE
lambda_1se
```

When applying the 1SE rule, the best lambda is 8103.084.

# (c) Fit a partial least squares model on the training data and report the test error. How many components are included in your model?

```{r}

```

# (d) Choose the best model for predicting the response and explain your choice.

```{r}

```

# (e) If “caret” was used for the elastic net in (b), retrain this model with “tidymodels”, and vice versa. Compare the selected tuning parameters between the two software approaches. Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.

```{r}
set.seed(2)
cv_folds <- vfold_cv(training_data, v = 10) 

enet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

# enet_spec %>% extract_parameter_dials("mixture")

enet_grid_set <- parameters(penalty(range = c(-5,9), trans = log_trans()),
                            mixture(range = c(0, 1)))
enet_grid <- grid_regular(enet_grid_set, levels = c(100, 21))

enet_workflow <- workflow() %>%
  add_model(enet_spec) %>%
  add_formula(Sale_Price ~ .)


enet_tune <- tune_grid(
  enet_workflow,
  resamples = cv_folds,
  grid = enet_grid
)


enet_best <- select_best(enet_tune, metric = "rmse") 

final_enet_spec <- enet_spec %>% 
  update(penalty = enet_best$penalty, mixture = enet_best$mixture)

enet_fit <- fit(final_enet_spec, formula = Sale_Price ~ ., data = training_data)
```

```{r}
enet_best$penalty
enet_best$mixture
```

```{r}
enet_pred <- predict(enet_fit, new_data = testing_data)

# Calculate test RMSE
sqrt(mean((enet_pred[[1]] - testing_data$Sale_Price)^2))
```

```{r}
# Extract results and apply the 1SE rule
enet_results <- enet_tune %>% collect_metrics()
min_rmse <- min(enet_results$mean[enet_results$.metric == "rmse"])
min_rmse_se <- enet_results$std_err[enet_results$mean == min_rmse]

lambda_within_1se <- enet_results$penalty[enet_results$mean <= (min_rmse + min_rmse_se)]
alpha_within_1se <- enet_results$mixture[enet_results$mean <= (min_rmse + min_rmse_se)]

lambda_1se <- max(lambda_within_1se)
alpha_1se_candidates <- unique(alpha_within_1se[enet_results$penalty == lambda_1se])

lambda_1se 
```


The selected lambda is 732, and the selected mixture is 0.05. The Test Error is 20903.5. The lambda selected by the tydimodels(732) is different from caret (272.0703). And the selected alpha is different as well. However, when applying 1SE rule, the selected lambda is 8103.084 by tidymodels, which is the same with caret. The difference in selected tuning parameters (alpha and lambda) suggests that the optimization process and the cross-validation strategy might vary between caret and tidymodels. Each package may have different defaults for handling data splitting, grid search strategies, or even in the underlying implementation of the glmnet function. But the same lambda under 1se rule suggests that simpler models within a reasonable range of the minimum error can often provide similar performance with the benefit of being more interpretable and less likely to overfit.















